\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Illustration of convolutional neural networks with shared weights.\relax }}{5}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces An example CNN architecture for image classification. Source: Stanford CS231n\relax }}{6}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces A recurrent neural network and the unfolding form. Source: Nature\relax }}{8}{figure.2.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Identity feature learning using different loss functions.\relax }}{14}{figure.3.1}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {Spatiotemporal Attention.} In challenging video search scenarios, a person is rarely fully visible in all frames. However, frames in which only part of the person is visible often contain useful information. For example, the face is clearly visible in the frames 1 and 2, the torso in frame 2, and the handbag in frames 2, 3 and $N$. Instead of averaging full frame features across time, we propose a new spatiotemporal approach which learns to detect a set of $K$ diverse salient image regions within each frame (superimposed heatmaps). An aggregate representation of each body part is then produced by combining the extracted per-frame regions across time (weights shown as white text). Our spatiotemporal approach creates a compact encoding of the video that exploits useful partial information in each frame by leveraging multiple spatial attention models, and combining their outputs using multiple temporal attention models.\relax }}{20}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf {Spatiotemporal Attention Network Architecture}. The input video is reduced to $N$ frames using restricted random sampling. (1) Each image is transformed into feature maps using a CNN. (2) These feature maps are sent to a conventional network followed by a softmax function to generate multiple spatial attention models and corresponding receptive fields for each input image. A diversity regularization term encourages learning spatial attention models that do not result in overlapping receptive fields per image. Each spatial attention model discovers a specific salient image region and generates a spatial gated feature (Fig.\nobreakspace {}\ref {vis}). (3) Spatial gated features from all frames are grouped by spatial attention model. (4) Temporal attentions compute an aggregate representation for the set of features generated by each spatial attention model. Finally, the spatiotemporal gated features for all body parts are concatenated into a single feature which represents the information contained in the entire video sequence. \relax }}{23}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf {Learned Spatial Attention Models}. Example images and corresponding receptive fields for our diverse spatial attention models when $K=6$. Our methodology discovers distinctive image regions which are useful for person search. The attention models primarily focus on foreground regions and generally correspond to specifc body parts. Our interpretation of each is indicated at the bottom of each column.\relax }}{26}{figure.4.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Given the natural language description of a person, our person search system searches through a large-scale person database then retrieve the most relevant person samples. \relax }}{36}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Example sentence descriptions from our dataset that describe persons' appearances in detail.\relax }}{40}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces High-frequency words and person images in our dataset.\relax }}{42}{figure.5.3}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Top-1 accuracy, top-5 accuracy, and average used time of manual person search using language descriptions with different number of sentences and different sentence lengths.\relax }}{42}{figure.5.4}
\contentsline {figure}{\numberline {5.5}{\ignorespaces The network structure of the proposed GNA-RNN. It consists of a visual sub-network (right blue branch) and a language sub-network (left branch). The visual sub-network generates a series of visual units, each of which encodes if certain appearance patterns exist in the person image. Given each input word, The language sub-network outputs world-level gates and unit-level attentions for weighting visual units.\relax }}{44}{figure.5.5}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Examples of top-6 person search results with natural language description by our proposed GNA-RNN. Corresponding images are marked by green rectangles. Successful searches where corresponding persons are in the top-6 results.\relax }}{53}{figure.5.6}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Examples of top-6 person search results with natural language description by our proposed GNA-RNN. Corresponding images are marked by green rectangles. Failure cases where corresponding persons are not in the top-6 results.\relax }}{53}{figure.5.7}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Images with the highest activations on 4 different visual units. The 4 units are identified as the one with the maximum average attention values in our GNA-RNN with the same word (``backpack'', ``sleeveless'', ``pink'', ``yellow'') and a large number of images. Each unit determines the existence of some common visual patterns.\relax }}{54}{figure.5.8}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Learning deep features for textual-visual matching with identity-level annotations. Utilizing identity-level annotations could jointly minimize intra-identity discrepancy and maximize inter-identity discrepancy, and thus results in more discriminative feature representations.\relax }}{56}{figure.6.1}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Illustration of the stage-1 network. In each iteration, the images and text descriptions in a mini-batch are first fed into the CNN and LSTM respectively to generate their feature representations. The CMCE loss is then computed by comparing sampled features in one modality to all other features in the feature buffer of the other modality (Step-1). The CNN and LSTM parameters are updated by backpropagation. Finally, the visual and textual feature buffers are updated with the sampled features (Step-2).\relax }}{60}{figure.6.2}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Illustration of the stage-2 network with latent co-attention mechanism. The spatial attention associates the relevant visual regions to each input word while the latent semantic attention automatically aligns image-word features by the spatial attention modules to enhance the robustness to sentence structure variations.\relax }}{63}{figure.6.3}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Example text-to-image retrieval results by the proposed framework. Corresponding images are marked by green rectangles. (Left to right) For each text description, the matching results are sorted according to the similarity scores in a descending order. (Row 1) results from the CUHK-Â­PEDES dataset \cite {li2017person}. (Row 2) results from the CUB dataset \cite {reed2016learning}. (Row 3) results from the Flower dataset \cite {reed2016learning}.\relax }}{73}{figure.6.4}
\addvspace {10\p@ }
