\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/R>>}
\providecommand \oddpage@label [2]{}
\HyPL@Entry{1<</S/r>>}
\@writefile{toc}{\contentsline {chapter}{Abstract}{ii}{chapter*.1}}
\@writefile{toc}{\contentsline {chapter}{Abstract in Chinese}{iii}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{iv}{chapter*.3}}
\@writefile{toc}{\contentsline {chapter}{Declaration}{v}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{Contents}{viii}{chapter*.5}}
\citation{li2017person}
\citation{reed2016learning}
\citation{reed2016learning}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{xi}{chapter*.6}}
\citation{li2017person}
\citation{reed2016learning}
\citation{reed2016learning}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xiii}{chapter*.7}}
\citation{lowe2004distinctive,dalal2005histograms,farenzena2010person,liao2015person}
\citation{krizhevsky2012imagenet,simonyan2014very,szegedy2014going,he2015deep}
\HyPL@Entry{15<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Learning Basics}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:deep-learning-basics}{{2}{4}{Deep Learning Basics}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convolutional Neural Network}{4}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Illustration of convolutional neural networks with shared weights.\relax }}{5}{figure.2.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{dl:cnn}{{2.1}{5}{Illustration of convolutional neural networks with shared weights.\relax }{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Convolutional Layer}{5}{subsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An example CNN architecture for image classification. Source: Stanford CS231n\relax }}{6}{figure.2.2}}
\newlabel{dl:cls}{{2.2}{6}{An example CNN architecture for image classification. Source: Stanford CS231n\relax }{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Fully-Connected Layer}{6}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}ReLU Layer and Softmax Layer}{7}{subsection.2.1.3}}
\newlabel{eq:softmax}{{2.5}{7}{ReLU Layer and Softmax Layer}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Pooling Layer}{7}{subsection.2.1.4}}
\citation{hochreiter1997long}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A recurrent neural network and the unfolding form. Source: Nature\relax }}{8}{figure.2.3}}
\newlabel{dl:rnn}{{2.3}{8}{A recurrent neural network and the unfolding form. Source: Nature\relax }{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Recurrent Neural Network}{8}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Long Short-Term Memory}{8}{subsection.2.2.1}}
\citation{cho2014learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Gated Recurrent Unit}{9}{subsection.2.2.2}}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{szegedy2015going}
\citation{he2016deep}
\citation{girshick2014rich}
\citation{girshick2014rich}
\citation{ren2015faster}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Learning Applications}{10}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Image Classification}{10}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Object Detection}{10}{subsection.2.3.2}}
\citation{radford2015unsupervised}
\citation{arjovsky2017wasserstein}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Generative Adversarial Networks}{11}{subsection.2.3.3}}
\citation{li2014deepreid,ahmed2015improved}
\citation{prosser2010person,gray2008viewpoint,liao2015efficient}
\citation{koestinger2012large,farenzena2010person,bakone,pedagadi2013local,ma2012local,xiao2017joint,kviatkovsky2013color,xiao2016end}
\citation{ahmed2015improved}
\citation{ding2015deep}
\citation{hamdoun2008person,prosser2008multi,li2015locality}
\citation{mclaughlin2016recurrent,zhousee,li2018diversity}
\citation{mclaughlin2016recurrent,chung2017two}
\citation{mclaughlin2016recurrent}
\citation{chung2017two}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Person Search Background}{12}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Person Search Categories}{12}{section.3.1}}
\newlabel{ps:reid}{{3.1}{12}{Person Search Categories}{section.3.1}{}}
\citation{xiao2016end,liao2015person,kumar2009attribute,xiao2016learning}
\citation{ahmed2015improved,masi2016pose}
\citation{cheng2016person,schroff2015facenet}
\citation{xiao2017joint}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Person Search Methods}{13}{section.3.2}}
\newlabel{ps:method}{{3.2}{13}{Person Search Methods}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Identity Feature Learning}{13}{subsection.3.2.1}}
\citation{davis2007information,weinberger2005distance,mcfee2010metric,koestinger2012large,liao2015person,zhang2016learning}
\citation{li2014deepreid}
\citation{ahmed2015improved}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Identity feature learning using different loss functions.\relax }}{14}{figure.3.1}}
\newlabel{fig:loss}{{3.1}{14}{Identity feature learning using different loss functions.\relax }{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Feature Comparison}{14}{subsection.3.2.2}}
\citation{gao2016compact}
\citation{gray2007evaluating}
\citation{zheng2009associating}
\citation{loy2009multi}
\citation{baltieri20113dpes}
\citation{li2012human}
\citation{li2013locally}
\citation{li2014deepreid}
\citation{xiaoli2017joint}
\citation{zheng2015scalable}
\citation{zheng2017unlabeled}
\citation{zheng2017unlabeled}
\citation{hirzer2011person}
\citation{wang2014person}
\citation{zheng2016mars}
\citation{gray2007evaluating}
\citation{loy2009multi}
\citation{zheng2009associating}
\citation{wang2014person}
\citation{li2012human}
\citation{li2013locally}
\citation{li2014deepreid}
\citation{li2014deepreid}
\citation{zheng2015scalable}
\citation{zheng2017unlabeled}
\citation{xiaoli2017joint}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Statistics of some commonly used datasets for person search.\relax }}{15}{table.3.1}}
\newlabel{tab:dataset}{{3.1}{15}{Statistics of some commonly used datasets for person search.\relax }{table.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Datasets}{15}{section.3.3}}
\newlabel{ps:dataset}{{3.3}{15}{Datasets}{section.3.3}{}}
\citation{hirzer2011person}
\citation{wang2014person}
\citation{zheng2016mars}
\citation{felzenszwalb2010object}
\citation{dehghan2015gmmcp}
\citation{zheng2016mars}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Evaluation Metrics}{16}{section.3.4}}
\newlabel{ps:evaluation}{{3.4}{16}{Evaluation Metrics}{section.3.4}{}}
\citation{zheng2015scalable}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Diversity Regularized Spatiotemporal Attention for Video-based Person Search}{18}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:person-reid}{{4}{18}{Diversity Regularized Spatiotemporal Attention for Video-based Person Search}{chapter.4}{}}
\citation{wang2013intelligent}
\citation{loy2009multi}
\citation{yu2013harry}
\citation{wang2013intelligent,shen2014multihuman}
\citation{gheissari2006person}
\citation{gray2008viewpoint,farenzena2010person,ma2012local,kviatkovsky2013color,xiao2016end}
\citation{prosser2010person,zheng2011person,koestinger2012large,pedagadi2013local,bakone}
\citation{ahmed2015improved,li2014deepreid,ding2015deep,li2017learning,xiao2017joint}
\citation{ahmed2015improved}
\citation{ding2015deep}
\citation{xiao2017joint}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Related Work}{19}{section.4.1}}
\citation{mclaughlin2016recurrent,zhousee,you2016top,wang2014person,zhu2016video,ma2017person}
\citation{you2016top}
\citation{mclaughlin2016recurrent}
\citation{wang2014person}
\citation{ma2017person}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  \textbf  {Spatiotemporal Attention.} In challenging video search scenarios, a person is rarely fully visible in all frames. However, frames in which only part of the person is visible often contain useful information. For example, the face is clearly visible in the frames 1 and 2, the torso in frame 2, and the handbag in frames 2, 3 and $N$. Instead of averaging full frame features across time, we propose a new spatiotemporal approach which learns to detect a set of $K$ diverse salient image regions within each frame (superimposed heatmaps). An aggregate representation of each body part is then produced by combining the extracted per-frame regions across time (weights shown as white text). Our spatiotemporal approach creates a compact encoding of the video that exploits useful partial information in each frame by leveraging multiple spatial attention models, and combining their outputs using multiple temporal attention models.\relax }}{20}{figure.4.1}}
\newlabel{intro}{{4.1}{20}{\textbf {Spatiotemporal Attention.} In challenging video search scenarios, a person is rarely fully visible in all frames. However, frames in which only part of the person is visible often contain useful information. For example, the face is clearly visible in the frames 1 and 2, the torso in frame 2, and the handbag in frames 2, 3 and $N$. Instead of averaging full frame features across time, we propose a new spatiotemporal approach which learns to detect a set of $K$ diverse salient image regions within each frame (superimposed heatmaps). An aggregate representation of each body part is then produced by combining the extracted per-frame regions across time (weights shown as white text). Our spatiotemporal approach creates a compact encoding of the video that exploits useful partial information in each frame by leveraging multiple spatial attention models, and combining their outputs using multiple temporal attention models.\relax }{figure.4.1}{}}
\citation{xu2015show,Li_2017_CVPR,Li_2017_ICCV}
\citation{xu2015show}
\citation{zhousee}
\citation{liu2017hydraplus}
\citation{zhousee,liu2017video,you2016top}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Method Overview}{21}{section.4.2}}
\citation{xiao2017joint}
\citation{mclaughlin2016recurrent,ma2017person,zhousee}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Restricted Random Sampling}{22}{section.4.3}}
\newlabel{sample}{{4.3}{22}{Restricted Random Sampling}{section.4.3}{}}
\citation{wang2016temporal}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {Spatiotemporal Attention Network Architecture}. The input video is reduced to $N$ frames using restricted random sampling. (1) Each image is transformed into feature maps using a CNN. (2) These feature maps are sent to a conventional network followed by a softmax function to generate multiple spatial attention models and corresponding receptive fields for each input image. A diversity regularization term encourages learning spatial attention models that do not result in overlapping receptive fields per image. Each spatial attention model discovers a specific salient image region and generates a spatial gated feature (Fig.\nobreakspace {}\ref  {vis}). (3) Spatial gated features from all frames are grouped by spatial attention model. (4) Temporal attentions compute an aggregate representation for the set of features generated by each spatial attention model. Finally, the spatiotemporal gated features for all body parts are concatenated into a single feature which represents the information contained in the entire video sequence. \relax }}{23}{figure.4.2}}
\newlabel{net2}{{4.2}{23}{\textbf {Spatiotemporal Attention Network Architecture}. The input video is reduced to $N$ frames using restricted random sampling. (1) Each image is transformed into feature maps using a CNN. (2) These feature maps are sent to a conventional network followed by a softmax function to generate multiple spatial attention models and corresponding receptive fields for each input image. A diversity regularization term encourages learning spatial attention models that do not result in overlapping receptive fields per image. Each spatial attention model discovers a specific salient image region and generates a spatial gated feature (Fig.~\ref {vis}). (3) Spatial gated features from all frames are grouped by spatial attention model. (4) Temporal attentions compute an aggregate representation for the set of features generated by each spatial attention model. Finally, the spatiotemporal gated features for all body parts are concatenated into a single feature which represents the information contained in the entire video sequence. \relax }{figure.4.2}{}}
\citation{he2016deep}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Multiple Spatial Attention Models}{24}{section.4.4}}
\newlabel{sec:mrsa}{{4.4}{24}{Multiple Spatial Attention Models}{section.4.4}{}}
\newlabel{eqn:spatial-activation}{{4.1}{24}{Multiple Spatial Attention Models}{equation.4.4.1}{}}
\citation{lin2017bilinear}
\citation{Carreira2012}
\newlabel{eqn:spatial-attention}{{4.3}{25}{Multiple Spatial Attention Models}{equation.4.4.3}{}}
\newlabel{eqn:enhancement}{{4.4}{25}{Multiple Spatial Attention Models}{equation.4.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Diversity Regularization}{25}{subsection.4.4.1}}
\newlabel{sec:diversity}{{4.4.1}{25}{Diversity Regularization}{subsection.4.4.1}{}}
\citation{lin2017structured}
\citation{beran1977minimum}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {Learned Spatial Attention Models}. Example images and corresponding receptive fields for our diverse spatial attention models when $K=6$. Our methodology discovers distinctive image regions which are useful for person search. The attention models primarily focus on foreground regions and generally correspond to specifc body parts. Our interpretation of each is indicated at the bottom of each column.\relax }}{26}{figure.4.3}}
\newlabel{vis}{{4.3}{26}{\textbf {Learned Spatial Attention Models}. Example images and corresponding receptive fields for our diverse spatial attention models when $K=6$. Our methodology discovers distinctive image regions which are useful for person search. The attention models primarily focus on foreground regions and generally correspond to specifc body parts. Our interpretation of each is indicated at the bottom of each column.\relax }{figure.4.3}{}}
\citation{lin2017structured}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Temporal Attention}{28}{section.4.5}}
\newlabel{sec:temporal}{{4.5}{28}{Temporal Attention}{section.4.5}{}}
\citation{xiao2017joint}
\citation{hirzer2011person}
\citation{wang2014person}
\citation{zheng2016mars}
\citation{felzenszwalb2010object}
\citation{dehghan2015gmmcp}
\newlabel{eqn:temporal-attention}{{4.13}{29}{Temporal Attention}{equation.4.5.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Re-Identification Loss}{29}{subsection.4.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Experiments}{29}{section.4.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Datasets}{29}{subsection.4.6.1}}
\citation{wang2014person}
\citation{zheng2016mars}
\citation{li2013locally}
\citation{li2014deepreid}
\citation{baltieri20113dpes}
\citation{gray2007evaluating}
\citation{zheng2017unlabeled}
\citation{xiao2017joint}
\citation{zheng2016mars}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Implementation details and evaluation metrics}{30}{subsection.4.6.2}}
\citation{lin2017structured}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Component analysis of the proposed method: rank-1 accuracies are reported. For MARS we provide mAP in brackets. \textbf  {SpaAtn} is the multi-region spatial attention, \textbf  {Q$'$} and \textbf  {Q} are two regularization terms, \textbf  {MaxPool} and \textbf  {TemAtn} are max temporal pooling and the proposed temporal attention respectively. \textbf  {Ind} represents fine-tuning the whole network to each dataset independently.\relax }}{31}{table.4.1}}
\newlabel{tab:component}{{4.1}{31}{Component analysis of the proposed method: rank-1 accuracies are reported. For MARS we provide mAP in brackets. \textbf {SpaAtn} is the multi-region spatial attention, \textbf {Q$'$} and \textbf {Q} are two regularization terms, \textbf {MaxPool} and \textbf {TemAtn} are max temporal pooling and the proposed temporal attention respectively. \textbf {Ind} represents fine-tuning the whole network to each dataset independently.\relax }{table.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Component Analysis of the Proposed Model}{31}{subsection.4.6.3}}
\newlabel{sec:component-analysis}{{4.6.3}{31}{Component Analysis of the Proposed Model}{subsection.4.6.3}{}}
\citation{liu2017hydraplus}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces The rank-1 accuracy using different number $K$ of diverse spatial attention models.\relax }}{32}{table.4.2}}
\newlabel{tab:number}{{4.2}{32}{The rank-1 accuracy using different number $K$ of diverse spatial attention models.\relax }{table.4.2}{}}
\citation{liu2015spatio}
\citation{karanam2015person}
\citation{you2016top}
\citation{zhu2016video}
\citation{chen2016person}
\citation{liu2017video}
\citation{mclaughlin2016recurrent}
\citation{zheng2016person}
\citation{liao2015person}
\citation{zheng2016mars}
\citation{liu2017video}
\citation{zheng2016mars}
\citation{zhousee}
\citation{liu2017quality}
\citation{khan2017multi}
\citation{khan2017multi}
\citation{zhousee}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Comparisons of our proposed approach to the state-of-the-art on PRID2011, iLIDS-VID, and MARS datasets. The rank-1 accuracies are reported and for MARS we provide mAP in brackets.\relax }}{33}{table.4.3}}
\newlabel{tab:statemethods}{{4.3}{33}{Comparisons of our proposed approach to the state-of-the-art on PRID2011, iLIDS-VID, and MARS datasets. The rank-1 accuracies are reported and for MARS we provide mAP in brackets.\relax }{table.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Comparison with the State-of-the-art Methods}{33}{subsection.4.6.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Conclusions}{33}{section.4.7}}
\citation{karpathy2015deep,vinyals2015show}
\citation{zhou2015simple,ren2015exploring}
\citation{reed2016learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Person Search with Natural Language Description}{35}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:person-search-nlp}{{5}{35}{Person Search with Natural Language Description}{chapter.5}{}}
\citation{zheng2011person,liao2015person,xiao2016end}
\citation{wang2013intelligent}
\citation{yu2013harry}
\citation{loy2009multi}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Given the natural language description of a person, our person search system searches through a large-scale person database then retrieve the most relevant person samples. \relax }}{36}{figure.5.1}}
\newlabel{fig:demo}{{5.1}{36}{Given the natural language description of a person, our person search system searches through a large-scale person database then retrieve the most relevant person samples. \relax }{figure.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Related Work}{36}{section.5.1}}
\newlabel{sec:ps-related-work}{{5.1}{36}{Related Work}{section.5.1}{}}
\citation{wang2007shape,hamdoun2008person,zhao2013unsupervised}
\citation{prosser2010person,porikli2003inter,shen2015person}
\citation{zheng2011person,gray2008viewpoint,prosser2010person,paisitkriangkrai2015learning,liao2015efficient}
\citation{li2014deepreid}
\citation{ahmed2015improved}
\citation{ding2015deep}
\citation{cheng2016person}
\citation{xiao2016learning}
\citation{li2015multi}
\citation{zheng2015partial}
\citation{vaquero2009attribute,su2016deep}
\citation{deng2014pedestrian}
\citation{hodosh2013framing}
\citation{young2014image}
\citation{chen2015microsoft}
\citation{lin2014microsoft}
\citation{krishna2016visual}
\citation{welinder2010caltech}
\citation{nilsback2008automated}
\citation{krizhevsky2012imagenet,he2016deep}
\citation{kang2016object,kang2016t,kang2017object}
\citation{xu2015show,antol2015vqa,hu2016segmentation,johnson2015densecap,gao2015you,chen2014learning,fang2015captions}
\citation{mao2014deep}
\citation{vinyals2015show}
\citation{karpathy2015deep}
\citation{ren2015exploring,noh2015image,yang2015stacked,saito2016dualnet,malinowski2015ask,fukui2016multimodal}
\citation{yang2015stacked}
\citation{noh2015image}
\citation{frome2013devise,karpathy2015deep,reed2016learning,liu2015multi,ren2015image}
\citation{reed2016learning}
\citation{frome2013devise}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Dataset and Method Overview}{39}{section.5.2}}
\newlabel{sec:ps-overview}{{5.2}{39}{Dataset and Method Overview}{section.5.2}{}}
\citation{li2014deepreid}
\citation{zheng2015person}
\citation{xiao2016end}
\citation{gray2007evaluating}
\citation{li2012human}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Example sentence descriptions from our dataset that describe persons' appearances in detail.\relax }}{40}{figure.5.2}}
\newlabel{fig:dataset}{{5.2}{40}{Example sentence descriptions from our dataset that describe persons' appearances in detail.\relax }{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Benchmark for person search with natural language description}{40}{section.5.3}}
\citation{lin2014microsoft}
\citation{krishna2016visual}
\citation{liao2015person}
\citation{deng2014pedestrian}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Dataset statistics}{41}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}User study}{41}{subsection.5.3.2}}
\newlabel{sec:userstudy}{{5.3.2}{41}{User study}{subsection.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces High-frequency words and person images in our dataset.\relax }}{42}{figure.5.3}}
\newlabel{fig:wordcould}{{5.3}{42}{High-frequency words and person images in our dataset.\relax }{figure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Top-1 accuracy, top-5 accuracy, and average used time of manual person search using language descriptions with different number of sentences and different sentence lengths.\relax }}{42}{figure.5.4}}
\newlabel{fig:sentnumberlength}{{5.4}{42}{Top-1 accuracy, top-5 accuracy, and average used time of manual person search using language descriptions with different number of sentences and different sentence lengths.\relax }{figure.5.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Top-1 accuracy, top-5 accuracy, and average used time of manual person search results using the original sentences, and sentences with nouns, or adjectives, or verbs masked out.\relax }}{43}{table.5.1}}
\newlabel{tab:nouns}{{5.1}{43}{Top-1 accuracy, top-5 accuracy, and average used time of manual person search results using the original sentences, and sentences with nouns, or adjectives, or verbs masked out.\relax }{table.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The network structure of the proposed GNA-RNN. It consists of a visual sub-network (right blue branch) and a language sub-network (left branch). The visual sub-network generates a series of visual units, each of which encodes if certain appearance patterns exist in the person image. Given each input word, The language sub-network outputs world-level gates and unit-level attentions for weighting visual units.\relax }}{44}{figure.5.5}}
\newlabel{fig:GNARNN}{{5.5}{44}{The network structure of the proposed GNA-RNN. It consists of a visual sub-network (right blue branch) and a language sub-network (left branch). The visual sub-network generates a series of visual units, each of which encodes if certain appearance patterns exist in the person image. Given each input word, The language sub-network outputs world-level gates and unit-level attentions for weighting visual units.\relax }{figure.5.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}GNA-RNN model for pedestrian search}{44}{section.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Visual units}{45}{subsection.5.4.1}}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Attention over visual units}{46}{subsection.5.4.2}}
\newlabel{sec:neuralattention}{{5.4.2}{46}{Attention over visual units}{subsection.5.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Word-level gates for visual units}{47}{subsection.5.4.3}}
\newlabel{sec:wordgate}{{5.4.3}{47}{Word-level gates for visual units}{subsection.5.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Training scheme}{48}{subsection.5.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experiments}{48}{section.5.5}}
\newlabel{sec:ps-experiments}{{5.5}{48}{Experiments}{section.5.5}{}}
\citation{vinyals2015show}
\citation{karpathy2015deep}
\citation{hu2015natural}
\citation{antol2015vqa}
\citation{antol2015vqa}
\citation{zhou2015simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Dataset and evaluation metrics}{49}{subsection.5.5.1}}
\newlabel{exp:dataset}{{5.5.1}{49}{Dataset and evaluation metrics}{subsection.5.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Compared methods and baselines}{49}{subsection.5.5.2}}
\newlabel{exp:baseline}{{5.5.2}{49}{Compared methods and baselines}{subsection.5.5.2}{}}
\citation{reed2016learning}
\citation{zhou2015simple}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Quantitative results of the proposed GNA-RNN and compared methods on the proposed dataset.\relax }}{50}{table.5.2}}
\newlabel{tab:results}{{5.2}{50}{Quantitative results of the proposed GNA-RNN and compared methods on the proposed dataset.\relax }{table.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Quantitative results of GNA-RNN on the proposed dataset without VGG-16 pre-training, without world-level gates or without unit-level attentions.\relax }}{50}{table.5.3}}
\newlabel{tab:ablation}{{5.3}{50}{Quantitative results of GNA-RNN on the proposed dataset without VGG-16 pre-training, without world-level gates or without unit-level attentions.\relax }{table.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Top-1 and top-10 accuracies of GNA-RNN with different number of visual units.\relax }}{51}{table.5.4}}
\newlabel{tab:VN}{{5.4}{51}{Top-1 and top-10 accuracies of GNA-RNN with different number of visual units.\relax }{table.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Quantitative and qualitative results}{51}{subsection.5.5.3}}
\newlabel{exp:results}{{5.5.3}{51}{Quantitative and qualitative results}{subsection.5.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Examples of top-6 person search results with natural language description by our proposed GNA-RNN. Corresponding images are marked by green rectangles. Successful searches where corresponding persons are in the top-6 results.\relax }}{53}{figure.5.6}}
\newlabel{fig:finalexamples}{{5.6}{53}{Examples of top-6 person search results with natural language description by our proposed GNA-RNN. Corresponding images are marked by green rectangles. Successful searches where corresponding persons are in the top-6 results.\relax }{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Examples of top-6 person search results with natural language description by our proposed GNA-RNN. Corresponding images are marked by green rectangles. Failure cases where corresponding persons are not in the top-6 results.\relax }}{53}{figure.5.7}}
\newlabel{fig:finalexamplesfail}{{5.7}{53}{Examples of top-6 person search results with natural language description by our proposed GNA-RNN. Corresponding images are marked by green rectangles. Failure cases where corresponding persons are not in the top-6 results.\relax }{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Images with the highest activations on 4 different visual units. The 4 units are identified as the one with the maximum average attention values in our GNA-RNN with the same word (``backpack'', ``sleeveless'', ``pink'', ``yellow'') and a large number of images. Each unit determines the existence of some common visual patterns.\relax }}{54}{figure.5.8}}
\newlabel{fig:units}{{5.8}{54}{Images with the highest activations on 4 different visual units. The 4 units are identified as the one with the maximum average attention values in our GNA-RNN with the same word (``backpack'', ``sleeveless'', ``pink'', ``yellow'') and a large number of images. Each unit determines the existence of some common visual patterns.\relax }{figure.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Conclusions}{54}{section.5.6}}
\newlabel{sec:ps-conclusions}{{5.6}{54}{Conclusions}{section.5.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}From Natural Language based Person Search to Textual-Visual Matching}{55}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{mao2014explain,Yan_2015_CVPR,wang2016learning,reed2016learning}
\citation{palatucci2009zero,rohrbach2011evaluating,frome2013devise}
\citation{Antol_2015_ICCV,fukui2016multimodal,zhu2016visual7w,nam2016dual,lu2016hierarchical}
\citation{lin2014microsoft,krishna2016visual,Antol_2015_ICCV,young2014image,hodosh2013framing}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Learning deep features for textual-visual matching with identity-level annotations. Utilizing identity-level annotations could jointly minimize intra-identity discrepancy and maximize inter-identity discrepancy, and thus results in more discriminative feature representations.\relax }}{56}{figure.6.1}}
\newlabel{fig:intro}{{6.1}{56}{Learning deep features for textual-visual matching with identity-level annotations. Utilizing identity-level annotations could jointly minimize intra-identity discrepancy and maximize inter-identity discrepancy, and thus results in more discriminative feature representations.\relax }{figure.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Related Work}{56}{section.6.1}}
\citation{zhang2016learning,xiao2017joint,ahmed2015improved,liao2015person,zheng2016person,xiao2017joint,Zhao_2017_CVPR}
\citation{masi2016pose,schroff2015facenet}
\citation{liao2015person,kumar2009attribute,xiao2016learning}
\citation{ahmed2015improved,masi2016pose,schroff2015facenet,cheng2016person}
\citation{mao2014deep,vinyals2015show,karpathy2015deep,Chen_2015_CVPR}
\citation{Antol_2015_ICCV,zhu2016visual7w,nam2016dual,lu2016hierarchical}
\citation{frome2013devise,reed2016learning,klein2015associating,Yan_2015_CVPR,wang2016learning}
\citation{karpathy2015deep}
\citation{nam2016dual}
\citation{Yan_2015_CVPR}
\citation{klein2015associating}
\citation{reed2016learning}
\citation{li2017person}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Method Overview}{58}{section.6.2}}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Stage-1 CNN-LSTM with CMCE Loss}{59}{section.6.3}}
\citation{Antol_2015_ICCV,lu2016hierarchical}
\citation{wang2016learning,reed2016learning}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Illustration of the stage-1 network. In each iteration, the images and text descriptions in a mini-batch are first fed into the CNN and LSTM respectively to generate their feature representations. The CMCE loss is then computed by comparing sampled features in one modality to all other features in the feature buffer of the other modality (Step-1). The CNN and LSTM parameters are updated by backpropagation. Finally, the visual and textual feature buffers are updated with the sampled features (Step-2).\relax }}{60}{figure.6.2}}
\newlabel{fig:stage1}{{6.2}{60}{Illustration of the stage-1 network. In each iteration, the images and text descriptions in a mini-batch are first fed into the CNN and LSTM respectively to generate their feature representations. The CMCE loss is then computed by comparing sampled features in one modality to all other features in the feature buffer of the other modality (Step-1). The CNN and LSTM parameters are updated by backpropagation. Finally, the visual and textual feature buffers are updated with the sampled features (Step-2).\relax }{figure.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Cross-Modal Cross-Entropy Loss}{60}{subsection.6.3.1}}
\newlabel{eq:1}{{6.1}{61}{Cross-Modal Cross-Entropy Loss}{equation.6.3.1}{}}
\newlabel{eq:2}{{6.2}{61}{Cross-Modal Cross-Entropy Loss}{equation.6.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Illustration of the stage-2 network with latent co-attention mechanism. The spatial attention associates the relevant visual regions to each input word while the latent semantic attention automatically aligns image-word features by the spatial attention modules to enhance the robustness to sentence structure variations.\relax }}{63}{figure.6.3}}
\newlabel{fig:stage2}{{6.3}{63}{Illustration of the stage-2 network with latent co-attention mechanism. The spatial attention associates the relevant visual regions to each input word while the latent semantic attention automatically aligns image-word features by the spatial attention modules to enhance the robustness to sentence structure variations.\relax }{figure.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Stage-2 CNN-LSTM with Latent Co-attention}{63}{section.6.4}}
\citation{bahdanau2014neural,vinyals2015show}
\citation{vinyals2015show}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Encoder word-LSTM with spatial attention}{64}{subsection.6.4.1}}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Decoder LSTM with latent semantic attention}{65}{subsection.6.4.2}}
\newlabel{eq:3}{{6.10}{65}{Decoder LSTM with latent semantic attention}{equation.6.4.10}{}}
\citation{li2017person}
\citation{reed2016learning}
\citation{reed2016learning}
\citation{li2017person}
\citation{li2017person}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Experiments}{66}{section.6.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Datasets and evaluation metrics}{66}{subsection.6.5.1}}
\citation{reed2016learning}
\citation{reed2016learning}
\citation{simonyan2014very}
\citation{szegedy2015going}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Implementation details}{67}{subsection.6.5.2}}
\citation{Antol_2015_ICCV}
\citation{zhou2015simple}
\citation{vinyals2015show}
\citation{reed2016learning}
\citation{li2017person}
\citation{klein2015associating}
\citation{li2017person}
\citation{li2017person}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Text-to-image retrieval results by different compared methods on the CUHK-­PEDES dataset \cite  {li2017person}.\relax }}{68}{table.6.1}}
\newlabel{tab:reid_dataset}{{6.1}{68}{Text-to-image retrieval results by different compared methods on the CUHK-­PEDES dataset \cite {li2017person}.\relax }{table.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Ablation studies on different components of the proposed two-stage framework. ``w/o ID'': not using identity-level annotations. ``w/o SMA'': not using semantic attention. ``w/o SPA'': not using spatial attention. ``w/o stage-1'': not using stage-1 network for training initialization and easy result screening.\relax }}{68}{table.6.2}}
\newlabel{tab:ablationtexval}{{6.2}{68}{Ablation studies on different components of the proposed two-stage framework. ``w/o ID'': not using identity-level annotations. ``w/o SMA'': not using semantic attention. ``w/o SPA'': not using spatial attention. ``w/o stage-1'': not using stage-1 network for training initialization and easy result screening.\relax }{table.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Results on CUHK-PEDES dataset}{68}{subsection.6.5.3}}
\citation{li2017person}
\citation{klein2015associating}
\citation{reed2016learning}
\citation{Antol_2015_ICCV}
\citation{zhou2015simple}
\citation{vinyals2015show}
\citation{li2017person}
\citation{reed2016learning}
\citation{reed2016learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.4}Ablation studies}{69}{subsection.6.5.4}}
\citation{reed2016learning}
\citation{mikolov2013distributed}
\citation{harris1954distributional}
\citation{akata2015evaluation}
\citation{reed2016learning}
\citation{harris1954distributional}
\citation{mikolov2013distributed}
\citation{akata2015evaluation}
\citation{reed2016learning}
\citation{reed2016learning}
\citation{klein2015associating}
\citation{reed2016learning}
\citation{reed2016learning}
\citation{harris1954distributional}
\citation{mikolov2013distributed}
\citation{reed2016learning}
\citation{reed2016learning}
\citation{klein2015associating}
\citation{reed2016learning}
\citation{reed2016learning}
\citation{li2017person}
\citation{reed2016learning}
\citation{reed2016learning}
\citation{li2017person}
\citation{reed2016learning}
\citation{reed2016learning}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Image-to-text and text-to-image retrieval results by different compared methods on the CUB dataset \cite  {reed2016learning}.\relax }}{71}{table.6.3}}
\newlabel{tab:CUB_dataset}{{6.3}{71}{Image-to-text and text-to-image retrieval results by different compared methods on the CUB dataset \cite {reed2016learning}.\relax }{table.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.5}Results on the CUB and Flower datasets}{71}{subsection.6.5.5}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Image-to-text and text-to-image retrieval results by different compared methods on the Flower dataset \cite  {reed2016learning}.\relax }}{72}{table.6.4}}
\newlabel{tab:Flower_dataset}{{6.4}{72}{Image-to-text and text-to-image retrieval results by different compared methods on the Flower dataset \cite {reed2016learning}.\relax }{table.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.6}Qualitative results}{72}{subsection.6.5.6}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Conclusion}{72}{section.6.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Example text-to-image retrieval results by the proposed framework. Corresponding images are marked by green rectangles. (Left to right) For each text description, the matching results are sorted according to the similarity scores in a descending order. (Row 1) results from the CUHK-­PEDES dataset \cite  {li2017person}. (Row 2) results from the CUB dataset \cite  {reed2016learning}. (Row 3) results from the Flower dataset \cite  {reed2016learning}.\relax }}{73}{figure.6.4}}
\newlabel{fig:retrieval}{{6.4}{73}{Example text-to-image retrieval results by the proposed framework. Corresponding images are marked by green rectangles. (Left to right) For each text description, the matching results are sorted according to the similarity scores in a descending order. (Row 1) results from the CUHK-­PEDES dataset \cite {li2017person}. (Row 2) results from the CUB dataset \cite {reed2016learning}. (Row 3) results from the Flower dataset \cite {reed2016learning}.\relax }{figure.6.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{75}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusions}{{7}{75}{Conclusions}{chapter.7}{}}
\bibstyle{IEEEtran}
\bibdata{backmatter/references}
\bibcite{li2017person}{1}
\bibcite{reed2016learning}{2}
\bibcite{lowe2004distinctive}{3}
\bibcite{dalal2005histograms}{4}
\bibcite{farenzena2010person}{5}
\bibcite{liao2015person}{6}
\bibcite{krizhevsky2012imagenet}{7}
\bibcite{simonyan2014very}{8}
\bibcite{szegedy2014going}{9}
\bibcite{he2015deep}{10}
\bibcite{hochreiter1997long}{11}
\bibcite{cho2014learning}{12}
\bibcite{szegedy2015going}{13}
\bibcite{he2016deep}{14}
\bibcite{girshick2014rich}{15}
\bibcite{ren2015faster}{16}
\bibcite{radford2015unsupervised}{17}
\bibcite{arjovsky2017wasserstein}{18}
\bibcite{li2014deepreid}{19}
\bibcite{ahmed2015improved}{20}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{76}{chapter.7}}
\bibcite{prosser2010person}{21}
\bibcite{gray2008viewpoint}{22}
\bibcite{liao2015efficient}{23}
\bibcite{koestinger2012large}{24}
\bibcite{bakone}{25}
\bibcite{pedagadi2013local}{26}
\bibcite{ma2012local}{27}
\bibcite{xiao2017joint}{28}
\bibcite{kviatkovsky2013color}{29}
\bibcite{xiao2016end}{30}
\bibcite{ding2015deep}{31}
\bibcite{hamdoun2008person}{32}
\bibcite{prosser2008multi}{33}
\bibcite{li2015locality}{34}
\bibcite{mclaughlin2016recurrent}{35}
\bibcite{zhousee}{36}
\bibcite{li2018diversity}{37}
\bibcite{chung2017two}{38}
\bibcite{kumar2009attribute}{39}
\bibcite{xiao2016learning}{40}
\bibcite{masi2016pose}{41}
\bibcite{cheng2016person}{42}
\bibcite{schroff2015facenet}{43}
\bibcite{davis2007information}{44}
\bibcite{weinberger2005distance}{45}
\bibcite{mcfee2010metric}{46}
\bibcite{zhang2016learning}{47}
\bibcite{gao2016compact}{48}
\bibcite{gray2007evaluating}{49}
\bibcite{zheng2009associating}{50}
\bibcite{loy2009multi}{51}
\bibcite{baltieri20113dpes}{52}
\bibcite{li2012human}{53}
\bibcite{li2013locally}{54}
\bibcite{xiaoli2017joint}{55}
\bibcite{zheng2015scalable}{56}
\bibcite{zheng2017unlabeled}{57}
\bibcite{hirzer2011person}{58}
\bibcite{wang2014person}{59}
\bibcite{zheng2016mars}{60}
\bibcite{felzenszwalb2010object}{61}
\bibcite{dehghan2015gmmcp}{62}
\bibcite{wang2013intelligent}{63}
\bibcite{yu2013harry}{64}
\bibcite{shen2014multihuman}{65}
\bibcite{gheissari2006person}{66}
\bibcite{zheng2011person}{67}
\bibcite{li2017learning}{68}
\bibcite{you2016top}{69}
\bibcite{zhu2016video}{70}
\bibcite{ma2017person}{71}
\bibcite{xu2015show}{72}
\bibcite{Li_2017_CVPR}{73}
\bibcite{Li_2017_ICCV}{74}
\bibcite{liu2017hydraplus}{75}
\bibcite{liu2017video}{76}
\bibcite{wang2016temporal}{77}
\bibcite{lin2017bilinear}{78}
\bibcite{Carreira2012}{79}
\bibcite{lin2017structured}{80}
\bibcite{beran1977minimum}{81}
\bibcite{liu2015spatio}{82}
\bibcite{karanam2015person}{83}
\bibcite{chen2016person}{84}
\bibcite{zheng2016person}{85}
\bibcite{liu2017quality}{86}
\bibcite{khan2017multi}{87}
\bibcite{karpathy2015deep}{88}
\bibcite{vinyals2015show}{89}
\bibcite{zhou2015simple}{90}
\bibcite{ren2015exploring}{91}
\bibcite{wang2007shape}{92}
\bibcite{zhao2013unsupervised}{93}
\bibcite{porikli2003inter}{94}
\bibcite{shen2015person}{95}
\bibcite{paisitkriangkrai2015learning}{96}
\bibcite{li2015multi}{97}
\bibcite{zheng2015partial}{98}
\bibcite{vaquero2009attribute}{99}
\bibcite{su2016deep}{100}
\bibcite{deng2014pedestrian}{101}
\bibcite{hodosh2013framing}{102}
\bibcite{young2014image}{103}
\bibcite{chen2015microsoft}{104}
\bibcite{lin2014microsoft}{105}
\bibcite{krishna2016visual}{106}
\bibcite{welinder2010caltech}{107}
\bibcite{nilsback2008automated}{108}
\bibcite{kang2016object}{109}
\bibcite{kang2016t}{110}
\bibcite{kang2017object}{111}
\bibcite{antol2015vqa}{112}
\bibcite{hu2016segmentation}{113}
\bibcite{johnson2015densecap}{114}
\bibcite{gao2015you}{115}
\bibcite{chen2014learning}{116}
\bibcite{fang2015captions}{117}
\bibcite{mao2014deep}{118}
\bibcite{noh2015image}{119}
\bibcite{yang2015stacked}{120}
\bibcite{saito2016dualnet}{121}
\bibcite{malinowski2015ask}{122}
\bibcite{fukui2016multimodal}{123}
\bibcite{frome2013devise}{124}
\bibcite{liu2015multi}{125}
\bibcite{ren2015image}{126}
\bibcite{zheng2015person}{127}
\bibcite{hu2015natural}{128}
\bibcite{mao2014explain}{129}
\bibcite{Yan_2015_CVPR}{130}
\bibcite{wang2016learning}{131}
\bibcite{palatucci2009zero}{132}
\bibcite{rohrbach2011evaluating}{133}
\bibcite{Antol_2015_ICCV}{134}
\bibcite{zhu2016visual7w}{135}
\bibcite{nam2016dual}{136}
\bibcite{lu2016hierarchical}{137}
\bibcite{Zhao_2017_CVPR}{138}
\bibcite{Chen_2015_CVPR}{139}
\bibcite{klein2015associating}{140}
\bibcite{bahdanau2014neural}{141}
\bibcite{mikolov2013distributed}{142}
\bibcite{harris1954distributional}{143}
\bibcite{akata2015evaluation}{144}
